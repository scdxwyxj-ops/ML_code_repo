{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12132c0",
   "metadata": {},
   "source": [
    "# Q2 Credit Fraud: Traditional vs Deep Models\n",
    "\n",
    "This notebook compares classical machine learning models against neural networks on the credit card fraud dataset using shared preprocessing profiles and persists trained neural weights under `assets/snapshots`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1268aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "sys.path.append(str(Path(os.getcwd()).parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b555965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Mapping, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from data_processings import (\n",
    "    apply_base_preprocessing,\n",
    "    apply_feature_sets,\n",
    "    apply_post_split_transforms,\n",
    "    append_target,\n",
    "    balance_training_dataframe,\n",
    "    get_experiment_config,\n",
    "    get_preprocessing_config,\n",
    "    load_config,\n",
    "    load_credit_card_data,\n",
    "    select_feature_columns,\n",
    ")\n",
    "from models import build_model\n",
    "from models.neural import build_dataloader, build_neural_model, predict_proba, train_neural_model\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "ROOT = Path(os.getcwd()).parent\n",
    "SNAPSHOT_DIR = ROOT / \"assets\" / \"snapshots\"\n",
    "SNAPSHOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = load_config()\n",
    "EXPERIMENT_KEY = \"q2_credit_fraud\"\n",
    "EXPERIMENT_CFG = get_experiment_config(EXPERIMENT_KEY)\n",
    "DATASET_KEY = EXPERIMENT_CFG[\"dataset\"]\n",
    "PREPROCESSING_CFG = get_preprocessing_config(DATASET_KEY)\n",
    "DEEP_MODELS_CFG = CONFIG.get(\"deep_models\", {})\n",
    "\n",
    "FEATURE_SETS_FIXED = tuple(EXPERIMENT_CFG.get(\"feature_sets_fixed\") or [\"baseline\"])\n",
    "CLASSICAL_MODELS = tuple(EXPERIMENT_CFG.get(\"models\", []))\n",
    "NEURAL_MODELS = tuple(EXPERIMENT_CFG.get(\"neural_models\", []))\n",
    "SPLIT_CFG = EXPERIMENT_CFG.get(\"split\", {})\n",
    "METRICS = tuple(EXPERIMENT_CFG.get(\"metrics\", [\"accuracy\"]))\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "PROFILE_DATA_CACHE: Dict[str, Dict[str, object]] = {}\n",
    "\n",
    "\n",
    "def load_dataset() -> pd.DataFrame:\n",
    "    options = EXPERIMENT_CFG.get(\"dataset_options\", {})\n",
    "    return load_credit_card_data(\n",
    "        parse_dates=options.get(\"parse_dates\", False),\n",
    "        limit_rows=options.get(\"limit_rows\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def sanitize_features_target(features: pd.DataFrame, target: pd.Series) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    target_name = target.name or \"target\"\n",
    "    combined = pd.concat([features, target.rename(target_name)], axis=1)\n",
    "    combined = combined.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
    "    cleaned_target = combined[target_name].astype(int)\n",
    "    cleaned_features = combined.drop(columns=[target_name]).astype(np.float32)\n",
    "    return cleaned_features, cleaned_target\n",
    "\n",
    "\n",
    "def split_time_series_frame(\n",
    "    df: pd.DataFrame,\n",
    "    split_cfg: Mapping[str, object],\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    method = split_cfg.get(\"method\", \"time\")\n",
    "    if method != \"time\":\n",
    "        raise ValueError(f\"Unsupported split method: {method}\")\n",
    "    test_size = float(split_cfg.get(\"test_size\", 0.2))\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    split_idx = max(1, min(split_idx, len(df) - 1))\n",
    "    return df.iloc[:split_idx].copy(), df.iloc[split_idx:].copy()\n",
    "\n",
    "\n",
    "def prepare_labelled_frame(\n",
    "    base_df: pd.DataFrame,\n",
    "    feature_sets: Sequence[str],\n",
    "    profile_config: Mapping[str, object],\n",
    ") -> pd.DataFrame:\n",
    "    enriched = apply_feature_sets(\n",
    "        base_df,\n",
    "        DATASET_KEY,\n",
    "        feature_sets,\n",
    "        config_override=profile_config,\n",
    "    )\n",
    "    labelled = append_target(\n",
    "        enriched,\n",
    "        DATASET_KEY,\n",
    "        config_override=profile_config,\n",
    "    )\n",
    "    return labelled.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
    "\n",
    "\n",
    "def prepare_profile_data(raw_df: pd.DataFrame, profile_name: str) -> Dict[str, object] | None:\n",
    "    if profile_name in PROFILE_DATA_CACHE:\n",
    "        return PROFILE_DATA_CACHE[profile_name]\n",
    "\n",
    "    base_df, profile_config = apply_base_preprocessing(\n",
    "        raw_df,\n",
    "        DATASET_KEY,\n",
    "        profile_name=profile_name,\n",
    "    )\n",
    "    labelled = prepare_labelled_frame(base_df, FEATURE_SETS_FIXED, profile_config)\n",
    "    if len(labelled) < 50:\n",
    "        PROFILE_DATA_CACHE[profile_name] = None\n",
    "        return None\n",
    "\n",
    "    train_df, test_df = split_time_series_frame(labelled, SPLIT_CFG)\n",
    "    train_df = balance_training_dataframe(train_df, DATASET_KEY, config_override=profile_config)\n",
    "    train_df, test_df = apply_post_split_transforms(train_df, test_df, profile_config)\n",
    "\n",
    "    X_train, y_train = select_feature_columns(train_df, DATASET_KEY, config_override=profile_config)\n",
    "    X_test, y_test = select_feature_columns(test_df, DATASET_KEY, config_override=profile_config)\n",
    "\n",
    "    X_train, y_train = sanitize_features_target(X_train, y_train)\n",
    "    X_test, y_test = sanitize_features_target(X_test, y_test)\n",
    "\n",
    "    if len(X_train) < 50 or len(X_test) < 25:\n",
    "        PROFILE_DATA_CACHE[profile_name] = None\n",
    "        return None\n",
    "\n",
    "    X_test = X_test.reindex(columns=X_train.columns, fill_value=0.0)\n",
    "\n",
    "    entry = {\n",
    "        \"profile_config\": profile_config,\n",
    "        \"train_df\": train_df,\n",
    "        \"test_df\": test_df,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "    }\n",
    "    PROFILE_DATA_CACHE[profile_name] = entry\n",
    "    return entry\n",
    "\n",
    "\n",
    "def evaluate_classical_models(data_entry: Dict[str, object], profile_name: str) -> list[dict[str, object]]:\n",
    "    if not CLASSICAL_MODELS:\n",
    "        return []\n",
    "\n",
    "    X_train: pd.DataFrame = data_entry[\"X_train\"]\n",
    "    y_train: pd.Series = data_entry[\"y_train\"]\n",
    "    X_test: pd.DataFrame = data_entry[\"X_test\"]\n",
    "    y_test: pd.Series = data_entry[\"y_test\"]\n",
    "\n",
    "    records: list[dict[str, object]] = []\n",
    "    X_train_np = X_train.to_numpy(dtype=np.float64)\n",
    "    y_train_np = y_train.to_numpy()\n",
    "    X_test_np = X_test.to_numpy(dtype=np.float64)\n",
    "    y_test_np = y_test.to_numpy()\n",
    "\n",
    "    for model_key in CLASSICAL_MODELS:\n",
    "        model = build_model(model_key)\n",
    "        model.fit(X_train_np, y_train_np)\n",
    "        preds = model.predict(X_test_np)\n",
    "\n",
    "        result = {\n",
    "            \"preproc_profile\": profile_name,\n",
    "            \"model\": model_key,\n",
    "            \"model_type\": \"traditional\",\n",
    "            \"train_samples\": len(X_train_np),\n",
    "            \"test_samples\": len(X_test_np),\n",
    "            \"num_features\": X_train.shape[1],\n",
    "        }\n",
    "        if \"accuracy\" in METRICS:\n",
    "            result[\"accuracy\"] = accuracy_score(y_test_np, preds)\n",
    "        if \"f1\" in METRICS:\n",
    "            result[\"f1\"] = f1_score(y_test_np, preds, zero_division=0)\n",
    "        records.append(result)\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def stratified_class_weights(labels: pd.Series) -> torch.Tensor:\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    weights = total / (len(classes) * counts)\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def train_neural_profiles(raw_df: pd.DataFrame, overwrite: bool = False) -> list[dict[str, object]]:\n",
    "    logs: list[dict[str, object]] = []\n",
    "    profile_names = PREPROCESSING_CFG.get(EXPERIMENT_CFG.get(\"ablation_sets_key\")) or []\n",
    "\n",
    "    for profile_name in tqdm(profile_names, desc=\"Evaluation\", leave=False):\n",
    "        data_entry = prepare_profile_data(raw_df, profile_name)\n",
    "        if not data_entry:\n",
    "            continue\n",
    "\n",
    "        X_train: pd.DataFrame = data_entry[\"X_train\"]\n",
    "        y_train: pd.Series = data_entry[\"y_train\"]\n",
    "\n",
    "        input_dim = X_train.shape[1]\n",
    "        num_classes = int(y_train.nunique())\n",
    "\n",
    "        train_tensor = torch.from_numpy(X_train.to_numpy(dtype=np.float32))\n",
    "        label_tensor = torch.from_numpy(y_train.to_numpy(dtype=np.int64))\n",
    "\n",
    "        val_fraction = 0.2\n",
    "        split_idx = max(1, int(len(train_tensor) * (1 - val_fraction)))\n",
    "        if split_idx >= len(train_tensor):\n",
    "            split_idx = len(train_tensor) - 1\n",
    "        train_features = train_tensor[:split_idx]\n",
    "        train_labels = label_tensor[:split_idx]\n",
    "        val_features = train_tensor[split_idx:]\n",
    "        val_labels = label_tensor[split_idx:]\n",
    "\n",
    "        if len(val_features) == 0:\n",
    "            val_features = train_features.clone()\n",
    "            val_labels = train_labels.clone()\n",
    "\n",
    "        class_weights = stratified_class_weights(y_train)\n",
    "        pin_memory = DEVICE.type == \"cuda\"\n",
    "\n",
    "        for model_key in NEURAL_MODELS:\n",
    "            snapshot_path = SNAPSHOT_DIR / f\"{profile_name}_{model_key}.pth\"\n",
    "            history_path = SNAPSHOT_DIR / f\"{profile_name}_{model_key}_history.json\"\n",
    "            if snapshot_path.exists() and not overwrite:\n",
    "                continue\n",
    "\n",
    "            model, training_cfg = build_neural_model(model_key, input_dim, DEEP_MODELS_CFG, num_classes=num_classes)\n",
    "            train_loader = build_dataloader(train_features, train_labels, training_cfg.batch_size, shuffle=True, pin_memory=pin_memory)\n",
    "            val_loader = build_dataloader(val_features, val_labels, training_cfg.batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "            history = train_neural_model(\n",
    "                model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                epochs=training_cfg.epochs,\n",
    "                learning_rate=training_cfg.learning_rate,\n",
    "                class_weights=class_weights,\n",
    "                device=DEVICE,\n",
    "                progress_label=f\"Train {profile_name}:{model_key}\",\n",
    "                save_path=snapshot_path,\n",
    "                metadata={\n",
    "                    \"model_key\": model_key,\n",
    "                    \"profile\": profile_name,\n",
    "                    \"input_dim\": input_dim,\n",
    "                    \"num_classes\": num_classes,\n",
    "                    \"grad_clip\": training_cfg.grad_clip,\n",
    "                },\n",
    "                grad_clip=training_cfg.grad_clip,\n",
    "            )\n",
    "\n",
    "            with history_path.open(\"w\", encoding=\"utf-8\") as history_file:\n",
    "                json.dump(history, history_file, indent=2)\n",
    "\n",
    "            logs.append(\n",
    "                {\n",
    "                    \"profile\": profile_name,\n",
    "                    \"model\": model_key,\n",
    "                    \"epochs\": training_cfg.epochs,\n",
    "                    \"snapshot\": snapshot_path.name,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if DEVICE.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    return logs\n",
    "\n",
    "\n",
    "def evaluate_neural_models(data_entry: Dict[str, object], profile_name: str) -> list[dict[str, object]]:\n",
    "    if not NEURAL_MODELS:\n",
    "        return []\n",
    "\n",
    "    X_test: pd.DataFrame = data_entry[\"X_test\"]\n",
    "    y_test: pd.Series = data_entry[\"y_test\"]\n",
    "    input_dim = X_test.shape[1]\n",
    "    num_classes = int(data_entry[\"y_train\"].nunique())\n",
    "\n",
    "    X_test_tensor = torch.from_numpy(X_test.to_numpy(dtype=np.float32))\n",
    "    y_test_tensor = torch.from_numpy(y_test.to_numpy(dtype=np.int64))\n",
    "    pin_memory = DEVICE.type == \"cuda\"\n",
    "    test_loader = build_dataloader(X_test_tensor, y_test_tensor, batch_size=1024, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "    records: list[dict[str, object]] = []\n",
    "    for model_key in NEURAL_MODELS:\n",
    "        snapshot_path = SNAPSHOT_DIR / f\"{profile_name}_{model_key}.pth\"\n",
    "        if not snapshot_path.exists():\n",
    "            continue\n",
    "\n",
    "        payload = torch.load(snapshot_path, map_location=DEVICE)\n",
    "        metadata = payload.get(\"metadata\", {})\n",
    "        input_dim_meta = int(metadata.get(\"input_dim\", input_dim))\n",
    "        num_classes_meta = int(metadata.get(\"num_classes\", num_classes))\n",
    "\n",
    "        model, _ = build_neural_model(model_key, input_dim_meta, DEEP_MODELS_CFG, num_classes=num_classes_meta)\n",
    "        model.load_state_dict(payload[\"state_dict\"])\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "        probs = predict_proba(model, test_loader)\n",
    "        preds = torch.argmax(probs, dim=1).numpy()\n",
    "        y_true = y_test_tensor.numpy()\n",
    "\n",
    "        result = {\n",
    "            \"preproc_profile\": profile_name,\n",
    "            \"model\": model_key,\n",
    "            \"model_type\": \"neural\",\n",
    "            \"train_samples\": len(data_entry[\"X_train\"]),\n",
    "            \"test_samples\": len(X_test_tensor),\n",
    "            \"num_features\": input_dim_meta,\n",
    "        }\n",
    "        if \"accuracy\" in METRICS:\n",
    "            result[\"accuracy\"] = accuracy_score(y_true, preds)\n",
    "        if \"f1\" in METRICS:\n",
    "            result[\"f1\"] = f1_score(y_true, preds, zero_division=0)\n",
    "        records.append(result)\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def run_evaluation(raw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    profile_names = PREPROCESSING_CFG.get(EXPERIMENT_CFG.get(\"ablation_sets_key\")) or []\n",
    "    records: list[dict[str, object]] = []\n",
    "\n",
    "    for profile_name in tqdm(profile_names, desc=\"Evaluation\", leave=False):\n",
    "        data_entry = prepare_profile_data(raw_df, profile_name)\n",
    "        if not data_entry:\n",
    "            continue\n",
    "\n",
    "        records.extend(evaluate_classical_models(data_entry, profile_name))\n",
    "        records.extend(evaluate_neural_models(data_entry, profile_name))\n",
    "\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "171b91f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (200000, 31)\n"
     ]
    }
   ],
   "source": [
    "raw_df = load_dataset()\n",
    "PROFILE_DATA_CACHE.clear()\n",
    "print(f'Dataset shape: {raw_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16712796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d52578549a64f19b86d0862d8e73c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f851179b04fc4ca0bdaf0c9d703ae60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P0_baseline:mlp:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_logs = train_neural_profiles(raw_df, overwrite=False)\n",
    "training_logs_df = pd.DataFrame(training_logs)\n",
    "training_logs_df if not training_logs_df.empty else 'No neural training executed (snapshots already present).'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1274d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_results = run_evaluation(raw_df)\n",
    "sort_columns = [col for col in ['preproc_profile', 'model_type', 'model'] if col in ablation_results.columns]\n",
    "if sort_columns:\n",
    "    ablation_results = ablation_results.sort_values(sort_columns)\n",
    "ablation_results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee6410",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Neural model weights and per-epoch loss histories are stored under `assets/snapshots` and re-used on subsequent runs.\n",
    "- Set `overwrite=True` when calling `train_neural_profiles` to retrain snapshots.\n",
    "- Ensure PyTorch with CUDA is installed to take advantage of GPU acceleration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
