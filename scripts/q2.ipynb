{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e978adc1",
   "metadata": {},
   "source": [
    "# Q2 Credit Card Fraud: Structured Pipeline\n",
    "\n",
    "This notebook compares classical machine-learning models and neural networks on the credit card fraud dataset using the new transform pipeline described in `assets/configs/q2/*.json`. Each profile has its own configuration file specifying the sequence of transforms, so the code below stays identical regardless of the preprocessing recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e006637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "\n",
    "MAIN_PATH = Path(os.getcwd()).parent\n",
    "sys.path.append(str(MAIN_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d5a83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Mapping, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from data_processings.datasets import CreditCardDataset\n",
    "from data_processings.pipeline_builder import build_pipeline_from_config, load_pipeline_config\n",
    "from data_processings.transforms import DFXPipeline\n",
    "from models import build_model\n",
    "from models.neural import build_dataloader, build_neural_model, predict_proba, train_neural_model\n",
    "from data_processings import load_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5334c1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_DIR = MAIN_PATH / 'assets' / 'configs' / 'q2'\n",
    "SNAPSHOT_DIR = MAIN_PATH / 'assets' / 'snapshots'\n",
    "SNAPSHOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROFILE_CONFIGS = {\n",
    "    'P0_baseline': CONFIG_DIR / 'P0_baseline.json',\n",
    "    'P1_robust_with_winsorize': CONFIG_DIR / 'P1_robust_with_winsorize.json',\n",
    "    'P2_minmax_global': CONFIG_DIR / 'P2_minmax_global.json',\n",
    "    'P3_log_amount_no_scaling': CONFIG_DIR / 'P3_log_amount_no_scaling.json',\n",
    "}\n",
    "\n",
    "GLOBAL_CONFIG = load_config()\n",
    "DEEP_MODELS_CFG = GLOBAL_CONFIG.get('deep_models', {})\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if DEVICE.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03037e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_time_series_frame(df: pd.DataFrame, split_cfg: Mapping[str, object]) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    method = split_cfg.get('method', 'time')\n",
    "    if method != 'time':\n",
    "        raise ValueError('Only time-based splits are supported in this notebook')\n",
    "    test_size = float(split_cfg.get('test_size', 0.2))\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    split_idx = max(1, min(split_idx, len(df) - 1))\n",
    "    return df.iloc[:split_idx].copy(), df.iloc[split_idx:].copy()\n",
    "\n",
    "\n",
    "def prepare_train_test(\n",
    "    raw_df: pd.DataFrame,\n",
    "    pipeline: DFXPipeline,\n",
    "    split_cfg: Mapping[str, object],\n",
    "    target_column: str,\n",
    ") -> tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Split raw data, run the pipeline, and ensure the target column survives.\"\"\"\n",
    "    train_df, test_df = split_time_series_frame(raw_df, split_cfg)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    processed_train = pipeline.fit_transform(train_df)\n",
    "    processed_test = pipeline.transform(test_df)\n",
    "\n",
    "    if target_column not in processed_train.columns:\n",
    "        if target_column in train_df.columns and len(train_df) == len(processed_train):\n",
    "            processed_train[target_column] = train_df[target_column].to_numpy()\n",
    "        else:\n",
    "            available = \", \".join(processed_train.columns.tolist())\n",
    "            raise KeyError(\n",
    "                f\"Target column '{target_column}' missing after preprocessing. Available columns: {available}\"\n",
    "            )\n",
    "\n",
    "    if target_column not in processed_test.columns:\n",
    "        if target_column in test_df.columns and len(test_df) == len(processed_test):\n",
    "            processed_test[target_column] = test_df[target_column].to_numpy()\n",
    "        else:\n",
    "            available = \", \".join(processed_test.columns.tolist())\n",
    "            raise KeyError(\n",
    "                f\"Target column '{target_column}' missing after preprocessing (test set). Available columns: {available}\"\n",
    "            )\n",
    "\n",
    "    y_train = processed_train.pop(target_column).astype(int)\n",
    "    y_test = processed_test.pop(target_column).astype(int)\n",
    "    return processed_train, y_train, processed_test, y_test\n",
    "\n",
    "\n",
    "def evaluate_classical_models(\n",
    "    model_keys: Sequence[str],\n",
    "    metrics: Sequence[str],\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    profile_name: str,\n",
    ") -> list[dict[str, object]]:\n",
    "    records: list[dict[str, object]] = []\n",
    "    if not model_keys:\n",
    "        print(f\"[{profile_name}] No classical models configured; skipping classical evaluation.\")\n",
    "        return records\n",
    "\n",
    "    print(f\"[{profile_name}] Evaluating classical models: {', '.join(model_keys)}\")\n",
    "    X_train_np = X_train.to_numpy(dtype=np.float64)\n",
    "    y_train_np = y_train.to_numpy()\n",
    "    X_test_np = X_test.to_numpy(dtype=np.float64)\n",
    "    y_test_np = y_test.to_numpy()\n",
    "\n",
    "    for model_key in model_keys:\n",
    "        print(f\"  -> fitting {model_key}...\", end='', flush=True)\n",
    "        model = build_model(model_key)\n",
    "        model.fit(X_train_np, y_train_np)\n",
    "        preds = model.predict(X_test_np)\n",
    "        print(\" done.\")\n",
    "\n",
    "        record: dict[str, object] = {\n",
    "            'preproc_profile': profile_name,\n",
    "            'model': model_key,\n",
    "            'model_type': 'traditional',\n",
    "            'train_samples': len(X_train_np),\n",
    "            'test_samples': len(X_test_np),\n",
    "            'num_features': X_train.shape[1],\n",
    "        }\n",
    "        if 'accuracy' in metrics:\n",
    "            record['accuracy'] = accuracy_score(y_test_np, preds)\n",
    "        if 'f1' in metrics:\n",
    "            record['f1'] = f1_score(y_test_np, preds, zero_division=0)\n",
    "        records.append(record)\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def train_neural_models(\n",
    "    profile_name: str,\n",
    "    model_keys: Sequence[str],\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    metrics: Sequence[str],\n",
    ") -> list[dict[str, object]]:\n",
    "    logs: list[dict[str, object]] = []\n",
    "    if not model_keys:\n",
    "        print(f\"[{profile_name}] No neural models configured; skipping neural training.\")\n",
    "        return logs\n",
    "\n",
    "    X_tensor = torch.from_numpy(X_train.to_numpy(dtype=np.float32))\n",
    "    y_tensor = torch.from_numpy(y_train.to_numpy(dtype=np.int64))\n",
    "\n",
    "    if len(X_tensor) < 2:\n",
    "        print(f\"[{profile_name}] Not enough samples to train neural models; skipping.\")\n",
    "        return logs\n",
    "\n",
    "    val_fraction = 0.2\n",
    "    split_idx = max(1, int(len(X_tensor) * (1 - val_fraction)))\n",
    "    if split_idx >= len(X_tensor):\n",
    "        split_idx = len(X_tensor) - 1\n",
    "    train_features = X_tensor[:split_idx]\n",
    "    train_labels = y_tensor[:split_idx]\n",
    "    val_features = X_tensor[split_idx:]\n",
    "    val_labels = y_tensor[split_idx:]\n",
    "\n",
    "    if len(val_features) == 0:\n",
    "        val_features = train_features.clone()\n",
    "        val_labels = train_labels.clone()\n",
    "\n",
    "    class_values, counts = np.unique(train_labels.numpy(), return_counts=True)\n",
    "    if len(class_values) < 2:\n",
    "        print(f\"[{profile_name}] Neural training skipped: only one class present after balancing.\")\n",
    "        return logs\n",
    "    total = counts.sum()\n",
    "    weights = total / (len(class_values) * counts)\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "    pin_memory = DEVICE.type == 'cuda'\n",
    "    print(f\"[{profile_name}] Training neural models: {', '.join(model_keys)}\")\n",
    "\n",
    "    for model_key in model_keys:\n",
    "        if model_key not in DEEP_MODELS_CFG:\n",
    "            print(f\"  -> {model_key} not found in deep model config; skipping.\")\n",
    "            continue\n",
    "        deep_cfg = DEEP_MODELS_CFG[model_key]\n",
    "        snapshot_path = SNAPSHOT_DIR / f\"{profile_name}_{model_key}.pth\"\n",
    "        history_path = SNAPSHOT_DIR / f\"{profile_name}_{model_key}_history.json\"\n",
    "        if snapshot_path.exists():\n",
    "            print(f\"  -> {model_key}: snapshot exists, skipping training.\")\n",
    "            logs.append({\n",
    "                'profile': profile_name,\n",
    "                'model': model_key,\n",
    "                'status': 'skipped (snapshot found)',\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        print(f\"  -> training {model_key} ({train_features.shape[0]} samples)...\")\n",
    "        model, training_cfg = build_neural_model(model_key, X_train.shape[1], DEEP_MODELS_CFG, num_classes=len(class_values))\n",
    "        train_loader = build_dataloader(train_features, train_labels, training_cfg.batch_size, shuffle=True, pin_memory=pin_memory)\n",
    "        val_loader = build_dataloader(val_features, val_labels, training_cfg.batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "        history = train_neural_model(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            epochs=training_cfg.epochs,\n",
    "            learning_rate=training_cfg.learning_rate,\n",
    "            class_weights=class_weights,\n",
    "            device=DEVICE,\n",
    "            progress_label=f\"Train {profile_name}:{model_key}\",\n",
    "            save_path=snapshot_path,\n",
    "            metadata={\n",
    "                'model_key': model_key,\n",
    "                'profile': profile_name,\n",
    "                'input_dim': X_train.shape[1],\n",
    "                'num_classes': len(class_values),\n",
    "                'metrics': list(metrics),\n",
    "            },\n",
    "            grad_clip=training_cfg.grad_clip,\n",
    "        )\n",
    "\n",
    "        with history_path.open('w', encoding='utf-8') as handle:\n",
    "            json.dump(history, handle, indent=2)\n",
    "\n",
    "        logs.append({\n",
    "            'profile': profile_name,\n",
    "            'model': model_key,\n",
    "            'status': 'trained',\n",
    "            'epochs': training_cfg.epochs,\n",
    "            'snapshot': snapshot_path.name,\n",
    "        })\n",
    "\n",
    "        if DEVICE.type == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    return logs\n",
    "\n",
    "\n",
    "def evaluate_neural_models(\n",
    "    profile_name: str,\n",
    "    model_keys: Sequence[str],\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    metrics: Sequence[str],\n",
    ") -> list[dict[str, object]]:\n",
    "    records: list[dict[str, object]] = []\n",
    "    if not model_keys:\n",
    "        print(f\"[{profile_name}] No neural models configured; skipping neural evaluation.\")\n",
    "        return records\n",
    "\n",
    "    y_true = y_test.to_numpy()\n",
    "    test_tensor = torch.from_numpy(X_test.to_numpy(dtype=np.float32))\n",
    "    test_labels = torch.from_numpy(y_true.astype(np.int64))\n",
    "\n",
    "    pin_memory = DEVICE.type == 'cuda'\n",
    "    test_loader = build_dataloader(test_tensor, test_labels, batch_size=1024, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "    print(f\"[{profile_name}] Evaluating neural models: {', '.join(model_keys)}\")\n",
    "    for model_key in model_keys:\n",
    "        snapshot_path = SNAPSHOT_DIR / f\"{profile_name}_{model_key}.pth\"\n",
    "        if not snapshot_path.exists() or model_key not in DEEP_MODELS_CFG:\n",
    "            print(f\"  -> {model_key}: snapshot missing; skipping evaluation.\")\n",
    "            continue\n",
    "        payload = torch.load(snapshot_path, map_location=DEVICE)\n",
    "        metadata = payload.get('metadata', {})\n",
    "        model, _ = build_neural_model(\n",
    "            model_key,\n",
    "            input_dim=int(metadata.get('input_dim', X_test.shape[1])),\n",
    "            config=DEEP_MODELS_CFG,\n",
    "            num_classes=int(metadata.get('num_classes', len(np.unique(y_true)))),\n",
    "        )\n",
    "        model.load_state_dict(payload['state_dict'])\n",
    "        model = model.to(DEVICE)\n",
    "        probs = predict_proba(model, test_loader)\n",
    "        preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "        record: dict[str, object] = {\n",
    "            'preproc_profile': profile_name,\n",
    "            'model': model_key,\n",
    "            'model_type': 'neural',\n",
    "            'train_samples': None,\n",
    "            'test_samples': len(X_test),\n",
    "            'num_features': X_test.shape[1],\n",
    "        }\n",
    "        if 'accuracy' in metrics:\n",
    "            record['accuracy'] = accuracy_score(y_true, preds)\n",
    "        if 'f1' in metrics:\n",
    "            record['f1'] = f1_score(y_true, preds, zero_division=0)\n",
    "        records.append(record)\n",
    "\n",
    "    return records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7518265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (10000, 31)\n"
     ]
    }
   ],
   "source": [
    "dataset_loader = CreditCardDataset()\n",
    "base_config = load_pipeline_config(PROFILE_CONFIGS['P0_baseline'])\n",
    "raw_df = dataset_loader.load(base_config.get('dataset_options', {}))\n",
    "print(f'Dataset shape: {raw_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0676084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda9da20be4d43498ee58fef24f618d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Profiles:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P0_baseline] Evaluating classical models: logistic_regression, naive_bayes, decision_tree, svm, random_forest, gradient_boosting\n",
      "  -> fitting logistic_regression... done.\n",
      "  -> fitting naive_bayes... done.\n",
      "  -> fitting decision_tree... done.\n",
      "  -> fitting svm... done.\n",
      "  -> fitting random_forest... done.\n",
      "  -> fitting gradient_boosting... done.\n",
      "[P0_baseline] Training neural models: mlp, residual_mlp, transformer\n",
      "  -> training mlp (6400 samples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632bbdf123e54a2281831fce5b8a77df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P0_baseline:mlp:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> training residual_mlp (6400 samples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fa087861d904202bbd0a282c50b3804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P0_baseline:residual_mlp:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> training transformer (6400 samples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869928b6326743a28dfff13031719961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P0_baseline:transformer:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P0_baseline] Evaluating neural models: mlp, residual_mlp, transformer\n",
      "[P1_robust_with_winsorize] Evaluating classical models: logistic_regression, naive_bayes, decision_tree, svm, random_forest, gradient_boosting\n",
      "  -> fitting logistic_regression... done.\n",
      "  -> fitting naive_bayes... done.\n",
      "  -> fitting decision_tree... done.\n",
      "  -> fitting svm... done.\n",
      "  -> fitting random_forest... done.\n",
      "  -> fitting gradient_boosting... done.\n",
      "[P1_robust_with_winsorize] Training neural models: mlp, residual_mlp, transformer\n",
      "  -> training mlp (6400 samples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46dcd3f0a0cc406a9f08c1d11c1cc733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P1_robust_with_winsorize:mlp:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> training residual_mlp (6400 samples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edaf280d5a4d42dbbe9fd9a954a4ff1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P1_robust_with_winsorize:residual_mlp:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> training transformer (6400 samples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45470224a3d1466db7001214f517ded9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P1_robust_with_winsorize:transformer:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P1_robust_with_winsorize] Evaluating neural models: mlp, residual_mlp, transformer\n",
      "[P2_minmax_global] Evaluating classical models: logistic_regression, naive_bayes, decision_tree, svm, random_forest, gradient_boosting\n",
      "  -> fitting logistic_regression... done.\n",
      "  -> fitting naive_bayes... done.\n",
      "  -> fitting decision_tree... done.\n",
      "  -> fitting svm... done.\n",
      "  -> fitting random_forest... done.\n",
      "  -> fitting gradient_boosting... done.\n",
      "[P2_minmax_global] Training neural models: mlp, residual_mlp, transformer\n",
      "  -> training mlp (6400 samples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51e31a68fea40468ae4e481adca61d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P2_minmax_global:mlp:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> training residual_mlp (6400 samples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb77079ed034164927bdd94e521661d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P2_minmax_global:residual_mlp:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> training transformer (6400 samples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff793523885c423d925d52c2e8b826a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P2_minmax_global:transformer:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P2_minmax_global] Evaluating neural models: mlp, residual_mlp, transformer\n",
      "[P3_log_amount_no_scaling] Evaluating classical models: logistic_regression, naive_bayes, decision_tree, svm, random_forest, gradient_boosting\n",
      "  -> fitting logistic_regression... done.\n",
      "  -> fitting naive_bayes... done.\n",
      "  -> fitting decision_tree... done.\n",
      "  -> fitting svm... done.\n",
      "  -> fitting random_forest... done.\n",
      "  -> fitting gradient_boosting... done.\n",
      "[P3_log_amount_no_scaling] Training neural models: mlp, residual_mlp, transformer\n",
      "  -> training mlp (6400 samples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c9515fa5e54c219211a0cd777c6b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P3_log_amount_no_scaling:mlp:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> training residual_mlp (6400 samples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e980a1307ed84003a7ec8ee30cdad309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P3_log_amount_no_scaling:residual_mlp:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> training transformer (6400 samples)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad21f9a137ef4df8a9b425cf924fc0ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P3_log_amount_no_scaling:transformer:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[P3_log_amount_no_scaling] Evaluating neural models: mlp, residual_mlp, transformer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preproc_profile</th>\n",
       "      <th>model</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_samples</th>\n",
       "      <th>test_samples</th>\n",
       "      <th>num_features</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>0.604651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>decision_tree</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>svm</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.470588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>gradient_boosting</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>mlp</td>\n",
       "      <td>neural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>residual_mlp</td>\n",
       "      <td>neural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>transformer</td>\n",
       "      <td>neural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9410</td>\n",
       "      <td>0.169014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>0.490566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>decision_tree</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>svm</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.928571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>gradient_boosting</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>mlp</td>\n",
       "      <td>neural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.742857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>residual_mlp</td>\n",
       "      <td>neural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.787879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>transformer</td>\n",
       "      <td>neural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9600</td>\n",
       "      <td>0.215686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>0.604651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>decision_tree</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>svm</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>gradient_boosting</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>mlp</td>\n",
       "      <td>neural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.764706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>residual_mlp</td>\n",
       "      <td>neural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>transformer</td>\n",
       "      <td>neural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.962963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>decision_tree</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>svm</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9935</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>gradient_boosting</td>\n",
       "      <td>traditional</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.774194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>mlp</td>\n",
       "      <td>neural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.012916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>residual_mlp</td>\n",
       "      <td>neural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9610</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>transformer</td>\n",
       "      <td>neural</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.9865</td>\n",
       "      <td>0.490566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             preproc_profile                model   model_type  train_samples  \\\n",
       "0                P0_baseline  logistic_regression  traditional         8000.0   \n",
       "1                P0_baseline          naive_bayes  traditional         8000.0   \n",
       "2                P0_baseline        decision_tree  traditional         8000.0   \n",
       "3                P0_baseline                  svm  traditional         8000.0   \n",
       "4                P0_baseline        random_forest  traditional         8000.0   \n",
       "5                P0_baseline    gradient_boosting  traditional         8000.0   \n",
       "6                P0_baseline                  mlp       neural            NaN   \n",
       "7                P0_baseline         residual_mlp       neural            NaN   \n",
       "8                P0_baseline          transformer       neural            NaN   \n",
       "9   P1_robust_with_winsorize  logistic_regression  traditional         8000.0   \n",
       "10  P1_robust_with_winsorize          naive_bayes  traditional         8000.0   \n",
       "11  P1_robust_with_winsorize        decision_tree  traditional         8000.0   \n",
       "12  P1_robust_with_winsorize                  svm  traditional         8000.0   \n",
       "13  P1_robust_with_winsorize        random_forest  traditional         8000.0   \n",
       "14  P1_robust_with_winsorize    gradient_boosting  traditional         8000.0   \n",
       "15  P1_robust_with_winsorize                  mlp       neural            NaN   \n",
       "16  P1_robust_with_winsorize         residual_mlp       neural            NaN   \n",
       "17  P1_robust_with_winsorize          transformer       neural            NaN   \n",
       "18          P2_minmax_global  logistic_regression  traditional         8000.0   \n",
       "19          P2_minmax_global          naive_bayes  traditional         8000.0   \n",
       "20          P2_minmax_global        decision_tree  traditional         8000.0   \n",
       "21          P2_minmax_global                  svm  traditional         8000.0   \n",
       "22          P2_minmax_global        random_forest  traditional         8000.0   \n",
       "23          P2_minmax_global    gradient_boosting  traditional         8000.0   \n",
       "24          P2_minmax_global                  mlp       neural            NaN   \n",
       "25          P2_minmax_global         residual_mlp       neural            NaN   \n",
       "26          P2_minmax_global          transformer       neural            NaN   \n",
       "27  P3_log_amount_no_scaling  logistic_regression  traditional         8000.0   \n",
       "28  P3_log_amount_no_scaling          naive_bayes  traditional         8000.0   \n",
       "29  P3_log_amount_no_scaling        decision_tree  traditional         8000.0   \n",
       "30  P3_log_amount_no_scaling                  svm  traditional         8000.0   \n",
       "31  P3_log_amount_no_scaling        random_forest  traditional         8000.0   \n",
       "32  P3_log_amount_no_scaling    gradient_boosting  traditional         8000.0   \n",
       "33  P3_log_amount_no_scaling                  mlp       neural            NaN   \n",
       "34  P3_log_amount_no_scaling         residual_mlp       neural            NaN   \n",
       "35  P3_log_amount_no_scaling          transformer       neural            NaN   \n",
       "\n",
       "    test_samples  num_features  accuracy        f1  \n",
       "0           2000            30    0.9965  0.787879  \n",
       "1           2000            30    0.9915  0.604651  \n",
       "2           2000            30    0.9965  0.774194  \n",
       "3           2000            30    0.9955  0.470588  \n",
       "4           2000            30    0.9985  0.888889  \n",
       "5           2000            30    0.9965  0.774194  \n",
       "6           2000            30    0.9960  0.764706  \n",
       "7           2000            30    0.9960  0.750000  \n",
       "8           2000            30    0.9410  0.169014  \n",
       "9           2000            30    0.9980  0.866667  \n",
       "10          2000            30    0.9865  0.490566  \n",
       "11          2000            30    0.9965  0.774194  \n",
       "12          2000            30    0.9990  0.928571  \n",
       "13          2000            30    0.9985  0.888889  \n",
       "14          2000            30    0.9965  0.774194  \n",
       "15          2000            30    0.9955  0.742857  \n",
       "16          2000            30    0.9965  0.787879  \n",
       "17          2000            30    0.9600  0.215686  \n",
       "18          2000            30    0.9960  0.764706  \n",
       "19          2000            30    0.9915  0.604651  \n",
       "20          2000            30    0.9965  0.774194  \n",
       "21          2000            30    0.9940  0.454545  \n",
       "22          2000            30    0.9985  0.888889  \n",
       "23          2000            30    0.9965  0.774194  \n",
       "24          2000            30    0.9960  0.764706  \n",
       "25          2000            30    0.9970  0.812500  \n",
       "26          2000            30    0.9950  0.705882  \n",
       "27          2000            30    0.9995  0.962963  \n",
       "28          2000            30    0.9930  0.650000  \n",
       "29          2000            30    0.9965  0.774194  \n",
       "30          2000            30    0.9935  0.000000  \n",
       "31          2000            30    0.9985  0.888889  \n",
       "32          2000            30    0.9965  0.774194  \n",
       "33          2000            30    0.0065  0.012916  \n",
       "34          2000            30    0.9610  0.250000  \n",
       "35          2000            30    0.9865  0.490566  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_records = []\n",
    "training_logs = []\n",
    "\n",
    "for profile_name in tqdm(PROFILE_CONFIGS.keys(), desc='Profiles'):\n",
    "    cfg = load_pipeline_config(PROFILE_CONFIGS[profile_name])\n",
    "    pipeline, metadata = build_pipeline_from_config(cfg)\n",
    "    target_column = cfg.get('target_column', metadata.get('target_column', 'Class'))\n",
    "    split_cfg = cfg.get('split', {'method': 'time', 'test_size': 0.2})\n",
    "\n",
    "    X_train, y_train, X_test, y_test = prepare_train_test(raw_df, pipeline, split_cfg, target_column)\n",
    "\n",
    "    classical_records = evaluate_classical_models(cfg.get('models', []), cfg.get('metrics', ['accuracy']), X_train, y_train, X_test, y_test, profile_name)\n",
    "    all_records.extend(classical_records)\n",
    "\n",
    "    train_logs = train_neural_models(profile_name, cfg.get('neural_models', []), X_train, y_train, cfg.get('metrics', ['accuracy']))\n",
    "    training_logs.extend(train_logs)\n",
    "\n",
    "    neural_records = evaluate_neural_models(profile_name, cfg.get('neural_models', []), X_test, y_test, cfg.get('metrics', ['accuracy']))\n",
    "    all_records.extend(neural_records)\n",
    "\n",
    "results_df = pd.DataFrame(all_records)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bec36c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile</th>\n",
       "      <th>model</th>\n",
       "      <th>status</th>\n",
       "      <th>epochs</th>\n",
       "      <th>snapshot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>mlp</td>\n",
       "      <td>trained</td>\n",
       "      <td>20</td>\n",
       "      <td>P0_baseline_mlp.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>residual_mlp</td>\n",
       "      <td>trained</td>\n",
       "      <td>30</td>\n",
       "      <td>P0_baseline_residual_mlp.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>transformer</td>\n",
       "      <td>trained</td>\n",
       "      <td>25</td>\n",
       "      <td>P0_baseline_transformer.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>mlp</td>\n",
       "      <td>trained</td>\n",
       "      <td>20</td>\n",
       "      <td>P1_robust_with_winsorize_mlp.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>residual_mlp</td>\n",
       "      <td>trained</td>\n",
       "      <td>30</td>\n",
       "      <td>P1_robust_with_winsorize_residual_mlp.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>transformer</td>\n",
       "      <td>trained</td>\n",
       "      <td>25</td>\n",
       "      <td>P1_robust_with_winsorize_transformer.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>mlp</td>\n",
       "      <td>trained</td>\n",
       "      <td>20</td>\n",
       "      <td>P2_minmax_global_mlp.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>residual_mlp</td>\n",
       "      <td>trained</td>\n",
       "      <td>30</td>\n",
       "      <td>P2_minmax_global_residual_mlp.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>transformer</td>\n",
       "      <td>trained</td>\n",
       "      <td>25</td>\n",
       "      <td>P2_minmax_global_transformer.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>mlp</td>\n",
       "      <td>trained</td>\n",
       "      <td>20</td>\n",
       "      <td>P3_log_amount_no_scaling_mlp.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>residual_mlp</td>\n",
       "      <td>trained</td>\n",
       "      <td>30</td>\n",
       "      <td>P3_log_amount_no_scaling_residual_mlp.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>transformer</td>\n",
       "      <td>trained</td>\n",
       "      <td>25</td>\n",
       "      <td>P3_log_amount_no_scaling_transformer.pth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     profile         model   status  epochs  \\\n",
       "0                P0_baseline           mlp  trained      20   \n",
       "1                P0_baseline  residual_mlp  trained      30   \n",
       "2                P0_baseline   transformer  trained      25   \n",
       "3   P1_robust_with_winsorize           mlp  trained      20   \n",
       "4   P1_robust_with_winsorize  residual_mlp  trained      30   \n",
       "5   P1_robust_with_winsorize   transformer  trained      25   \n",
       "6           P2_minmax_global           mlp  trained      20   \n",
       "7           P2_minmax_global  residual_mlp  trained      30   \n",
       "8           P2_minmax_global   transformer  trained      25   \n",
       "9   P3_log_amount_no_scaling           mlp  trained      20   \n",
       "10  P3_log_amount_no_scaling  residual_mlp  trained      30   \n",
       "11  P3_log_amount_no_scaling   transformer  trained      25   \n",
       "\n",
       "                                     snapshot  \n",
       "0                         P0_baseline_mlp.pth  \n",
       "1                P0_baseline_residual_mlp.pth  \n",
       "2                 P0_baseline_transformer.pth  \n",
       "3            P1_robust_with_winsorize_mlp.pth  \n",
       "4   P1_robust_with_winsorize_residual_mlp.pth  \n",
       "5    P1_robust_with_winsorize_transformer.pth  \n",
       "6                    P2_minmax_global_mlp.pth  \n",
       "7           P2_minmax_global_residual_mlp.pth  \n",
       "8            P2_minmax_global_transformer.pth  \n",
       "9            P3_log_amount_no_scaling_mlp.pth  \n",
       "10  P3_log_amount_no_scaling_residual_mlp.pth  \n",
       "11   P3_log_amount_no_scaling_transformer.pth  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_logs_df = pd.DataFrame(training_logs); training_logs_df if not training_logs_df.empty else 'No neural training required.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd8bdf6",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Each profile uses its own configuration under `assets/configs/q2/`, making the preprocessing pipeline explicit and reusable.\n",
    "- Neural checkpoints are stored in `assets/snapshots/` and reused on subsequent runs unless removed."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
