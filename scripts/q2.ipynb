{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12132c0",
   "metadata": {},
   "source": [
    "# Q2 Credit Fraud: Traditional vs Deep Models\n",
    "\n",
    "This notebook compares classical machine learning models against neural networks on the credit card fraud dataset using shared preprocessing profiles and persists trained neural weights under `assets/snapshots`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1268aec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, sys\n",
    "sys.path.append(str(Path(os.getcwd()).parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b555965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Mapping, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from data_processings import (\n",
    "    apply_base_preprocessing,\n",
    "    apply_feature_sets,\n",
    "    apply_post_split_transforms,\n",
    "    append_target,\n",
    "    get_experiment_config,\n",
    "    get_preprocessing_config,\n",
    "    load_config,\n",
    "    load_credit_card_data,\n",
    "    select_feature_columns,\n",
    ")\n",
    "from models import build_model\n",
    "from models.neural import build_dataloader, build_neural_model, predict_proba, train_neural_model\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if DEVICE.type == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "ROOT = Path(os.getcwd()).parent\n",
    "SNAPSHOT_DIR = ROOT / \"assets\" / \"snapshots\"\n",
    "SNAPSHOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = load_config()\n",
    "EXPERIMENT_KEY = \"q2_credit_fraud\"\n",
    "EXPERIMENT_CFG = get_experiment_config(EXPERIMENT_KEY)\n",
    "DATASET_KEY = EXPERIMENT_CFG[\"dataset\"]\n",
    "PREPROCESSING_CFG = get_preprocessing_config(DATASET_KEY)\n",
    "DEEP_MODELS_CFG = CONFIG.get(\"deep_models\", {})\n",
    "\n",
    "FEATURE_SETS_FIXED = tuple(EXPERIMENT_CFG.get(\"feature_sets_fixed\") or [\"baseline\"])\n",
    "CLASSICAL_MODELS = tuple(EXPERIMENT_CFG.get(\"models\", []))\n",
    "NEURAL_MODELS = tuple(EXPERIMENT_CFG.get(\"neural_models\", []))\n",
    "SPLIT_CFG = EXPERIMENT_CFG.get(\"split\", {})\n",
    "METRICS = tuple(EXPERIMENT_CFG.get(\"metrics\", [\"accuracy\"]))\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "PROFILE_DATA_CACHE: Dict[str, Dict[str, object]] = {}\n",
    "\n",
    "\n",
    "def load_dataset() -> pd.DataFrame:\n",
    "    options = EXPERIMENT_CFG.get(\"dataset_options\", {})\n",
    "    return load_credit_card_data(\n",
    "        parse_dates=options.get(\"parse_dates\", False),\n",
    "        limit_rows=options.get(\"limit_rows\"),\n",
    "    )\n",
    "\n",
    "\n",
    "def sanitize_features_target(features: pd.DataFrame, target: pd.Series) -> tuple[pd.DataFrame, pd.Series]:\n",
    "    target_name = target.name or \"target\"\n",
    "    combined = pd.concat([features, target.rename(target_name)], axis=1)\n",
    "    combined = combined.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
    "    cleaned_target = combined[target_name].astype(int)\n",
    "    cleaned_features = combined.drop(columns=[target_name]).astype(np.float32)\n",
    "    return cleaned_features, cleaned_target\n",
    "\n",
    "\n",
    "def split_time_series_frame(\n",
    "    df: pd.DataFrame,\n",
    "    split_cfg: Mapping[str, object],\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    method = split_cfg.get(\"method\", \"time\")\n",
    "    if method != \"time\":\n",
    "        raise ValueError(f\"Unsupported split method: {method}\")\n",
    "    test_size = float(split_cfg.get(\"test_size\", 0.2))\n",
    "    split_idx = int(len(df) * (1 - test_size))\n",
    "    split_idx = max(1, min(split_idx, len(df) - 1))\n",
    "    return df.iloc[:split_idx].copy(), df.iloc[split_idx:].copy()\n",
    "\n",
    "\n",
    "def prepare_labelled_frame(\n",
    "    base_df: pd.DataFrame,\n",
    "    feature_sets: Sequence[str],\n",
    "    profile_config: Mapping[str, object],\n",
    ") -> pd.DataFrame:\n",
    "    enriched = apply_feature_sets(\n",
    "        base_df,\n",
    "        DATASET_KEY,\n",
    "        feature_sets,\n",
    "        config_override=profile_config,\n",
    "    )\n",
    "    labelled = append_target(\n",
    "        enriched,\n",
    "        DATASET_KEY,\n",
    "        config_override=profile_config,\n",
    "    )\n",
    "    return labelled.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
    "\n",
    "\n",
    "def prepare_profile_data(raw_df: pd.DataFrame, profile_name: str) -> Dict[str, object] | None:\n",
    "    if profile_name in PROFILE_DATA_CACHE:\n",
    "        return PROFILE_DATA_CACHE[profile_name]\n",
    "\n",
    "    base_df, profile_config = apply_base_preprocessing(\n",
    "        raw_df,\n",
    "        DATASET_KEY,\n",
    "        profile_name=profile_name,\n",
    "    )\n",
    "    labelled = prepare_labelled_frame(base_df, FEATURE_SETS_FIXED, profile_config)\n",
    "    if len(labelled) < 50:\n",
    "        PROFILE_DATA_CACHE[profile_name] = None\n",
    "        return None\n",
    "\n",
    "    train_df, test_df = split_time_series_frame(labelled, SPLIT_CFG)\n",
    "    train_df, test_df = apply_post_split_transforms(train_df, test_df, profile_config)\n",
    "\n",
    "    X_train, y_train = select_feature_columns(train_df, DATASET_KEY, config_override=profile_config)\n",
    "    X_test, y_test = select_feature_columns(test_df, DATASET_KEY, config_override=profile_config)\n",
    "\n",
    "    X_train, y_train = sanitize_features_target(X_train, y_train)\n",
    "    X_test, y_test = sanitize_features_target(X_test, y_test)\n",
    "\n",
    "    if len(X_train) < 50 or len(X_test) < 25:\n",
    "        PROFILE_DATA_CACHE[profile_name] = None\n",
    "        return None\n",
    "\n",
    "    X_test = X_test.reindex(columns=X_train.columns, fill_value=0.0)\n",
    "\n",
    "    entry = {\n",
    "        \"profile_config\": profile_config,\n",
    "        \"train_df\": train_df,\n",
    "        \"test_df\": test_df,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "    }\n",
    "    PROFILE_DATA_CACHE[profile_name] = entry\n",
    "    return entry\n",
    "\n",
    "\n",
    "def evaluate_classical_models(data_entry: Dict[str, object], profile_name: str) -> list[dict[str, object]]:\n",
    "    if not CLASSICAL_MODELS:\n",
    "        return []\n",
    "\n",
    "    X_train: pd.DataFrame = data_entry[\"X_train\"]\n",
    "    y_train: pd.Series = data_entry[\"y_train\"]\n",
    "    X_test: pd.DataFrame = data_entry[\"X_test\"]\n",
    "    y_test: pd.Series = data_entry[\"y_test\"]\n",
    "\n",
    "    records: list[dict[str, object]] = []\n",
    "    X_train_np = X_train.to_numpy(dtype=np.float64)\n",
    "    y_train_np = y_train.to_numpy()\n",
    "    X_test_np = X_test.to_numpy(dtype=np.float64)\n",
    "    y_test_np = y_test.to_numpy()\n",
    "\n",
    "    for model_key in CLASSICAL_MODELS:\n",
    "        model = build_model(model_key)\n",
    "        model.fit(X_train_np, y_train_np)\n",
    "        preds = model.predict(X_test_np)\n",
    "\n",
    "        result = {\n",
    "            \"preproc_profile\": profile_name,\n",
    "            \"model\": model_key,\n",
    "            \"model_type\": \"traditional\",\n",
    "            \"train_samples\": len(X_train_np),\n",
    "            \"test_samples\": len(X_test_np),\n",
    "            \"num_features\": X_train.shape[1],\n",
    "        }\n",
    "        if \"accuracy\" in METRICS:\n",
    "            result[\"accuracy\"] = accuracy_score(y_test_np, preds)\n",
    "        if \"f1\" in METRICS:\n",
    "            result[\"f1\"] = f1_score(y_test_np, preds, zero_division=0)\n",
    "        records.append(result)\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def stratified_class_weights(labels: pd.Series) -> torch.Tensor:\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    total = counts.sum()\n",
    "    weights = total / (len(classes) * counts)\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def train_neural_profiles(raw_df: pd.DataFrame, overwrite: bool = False) -> list[dict[str, object]]:\n",
    "    logs: list[dict[str, object]] = []\n",
    "    profile_names = PREPROCESSING_CFG.get(EXPERIMENT_CFG.get(\"ablation_sets_key\")) or []\n",
    "\n",
    "    for profile_name in profile_names:\n",
    "        data_entry = prepare_profile_data(raw_df, profile_name)\n",
    "        if not data_entry:\n",
    "            continue\n",
    "\n",
    "        X_train: pd.DataFrame = data_entry[\"X_train\"]\n",
    "        y_train: pd.Series = data_entry[\"y_train\"]\n",
    "\n",
    "        input_dim = X_train.shape[1]\n",
    "        num_classes = int(y_train.nunique())\n",
    "\n",
    "        train_tensor = torch.from_numpy(X_train.to_numpy(dtype=np.float32))\n",
    "        label_tensor = torch.from_numpy(y_train.to_numpy(dtype=np.int64))\n",
    "\n",
    "        val_fraction = 0.2\n",
    "        split_idx = max(1, int(len(train_tensor) * (1 - val_fraction)))\n",
    "        if split_idx >= len(train_tensor):\n",
    "            split_idx = len(train_tensor) - 1\n",
    "        train_features = train_tensor[:split_idx]\n",
    "        train_labels = label_tensor[:split_idx]\n",
    "        val_features = train_tensor[split_idx:]\n",
    "        val_labels = label_tensor[split_idx:]\n",
    "\n",
    "        if len(val_features) == 0:\n",
    "            val_features = train_features.clone()\n",
    "            val_labels = train_labels.clone()\n",
    "\n",
    "        class_weights = stratified_class_weights(y_train)\n",
    "        pin_memory = DEVICE.type == \"cuda\"\n",
    "\n",
    "        for model_key in NEURAL_MODELS:\n",
    "            snapshot_path = SNAPSHOT_DIR / f\"{profile_name}_{model_key}.pth\"\n",
    "            history_path = SNAPSHOT_DIR / f\"{profile_name}_{model_key}_history.json\"\n",
    "            if snapshot_path.exists() and not overwrite:\n",
    "                continue\n",
    "\n",
    "            model, training_cfg = build_neural_model(model_key, input_dim, DEEP_MODELS_CFG, num_classes=num_classes)\n",
    "            train_loader = build_dataloader(train_features, train_labels, training_cfg.batch_size, shuffle=True, pin_memory=pin_memory)\n",
    "            val_loader = build_dataloader(val_features, val_labels, training_cfg.batch_size, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "            history = train_neural_model(\n",
    "                model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                epochs=training_cfg.epochs,\n",
    "                learning_rate=training_cfg.learning_rate,\n",
    "                class_weights=class_weights,\n",
    "                device=DEVICE,\n",
    "                progress_label=f\"Train {profile_name}:{model_key}\",\n",
    "                save_path=snapshot_path,\n",
    "                metadata={\n",
    "                    \"model_key\": model_key,\n",
    "                    \"profile\": profile_name,\n",
    "                    \"input_dim\": input_dim,\n",
    "                    \"num_classes\": num_classes,\n",
    "                },\n",
    "            )\n",
    "\n",
    "            with history_path.open(\"w\", encoding=\"utf-8\") as history_file:\n",
    "                json.dump(history, history_file, indent=2)\n",
    "\n",
    "            logs.append(\n",
    "                {\n",
    "                    \"profile\": profile_name,\n",
    "                    \"model\": model_key,\n",
    "                    \"epochs\": training_cfg.epochs,\n",
    "                    \"snapshot\": snapshot_path.name,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if DEVICE.type == \"cuda\":\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    return logs\n",
    "\n",
    "\n",
    "def evaluate_neural_models(data_entry: Dict[str, object], profile_name: str) -> list[dict[str, object]]:\n",
    "    if not NEURAL_MODELS:\n",
    "        return []\n",
    "\n",
    "    X_test: pd.DataFrame = data_entry[\"X_test\"]\n",
    "    y_test: pd.Series = data_entry[\"y_test\"]\n",
    "    input_dim = X_test.shape[1]\n",
    "    num_classes = int(data_entry[\"y_train\"].nunique())\n",
    "\n",
    "    X_test_tensor = torch.from_numpy(X_test.to_numpy(dtype=np.float32))\n",
    "    y_test_tensor = torch.from_numpy(y_test.to_numpy(dtype=np.int64))\n",
    "    pin_memory = DEVICE.type == \"cuda\"\n",
    "    test_loader = build_dataloader(X_test_tensor, y_test_tensor, batch_size=1024, shuffle=False, pin_memory=pin_memory)\n",
    "\n",
    "    records: list[dict[str, object]] = []\n",
    "    for model_key in NEURAL_MODELS:\n",
    "        snapshot_path = SNAPSHOT_DIR / f\"{profile_name}_{model_key}.pth\"\n",
    "        if not snapshot_path.exists():\n",
    "            continue\n",
    "\n",
    "        payload = torch.load(snapshot_path, map_location=DEVICE)\n",
    "        metadata = payload.get(\"metadata\", {})\n",
    "        input_dim_meta = int(metadata.get(\"input_dim\", input_dim))\n",
    "        num_classes_meta = int(metadata.get(\"num_classes\", num_classes))\n",
    "\n",
    "        model, _ = build_neural_model(model_key, input_dim_meta, DEEP_MODELS_CFG, num_classes=num_classes_meta)\n",
    "        model.load_state_dict(payload[\"state_dict\"])\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "        probs = predict_proba(model, test_loader)\n",
    "        preds = torch.argmax(probs, dim=1).numpy()\n",
    "        y_true = y_test_tensor.numpy()\n",
    "\n",
    "        result = {\n",
    "            \"preproc_profile\": profile_name,\n",
    "            \"model\": model_key,\n",
    "            \"model_type\": \"neural\",\n",
    "            \"train_samples\": len(data_entry[\"X_train\"]),\n",
    "            \"test_samples\": len(X_test_tensor),\n",
    "            \"num_features\": input_dim_meta,\n",
    "        }\n",
    "        if \"accuracy\" in METRICS:\n",
    "            result[\"accuracy\"] = accuracy_score(y_true, preds)\n",
    "        if \"f1\" in METRICS:\n",
    "            result[\"f1\"] = f1_score(y_true, preds, zero_division=0)\n",
    "        records.append(result)\n",
    "\n",
    "    return records\n",
    "\n",
    "\n",
    "def run_evaluation(raw_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    profile_names = PREPROCESSING_CFG.get(EXPERIMENT_CFG.get(\"ablation_sets_key\")) or []\n",
    "    records: list[dict[str, object]] = []\n",
    "\n",
    "    for profile_name in profile_names:\n",
    "        data_entry = prepare_profile_data(raw_df, profile_name)\n",
    "        if not data_entry:\n",
    "            continue\n",
    "\n",
    "        records.extend(evaluate_classical_models(data_entry, profile_name))\n",
    "        records.extend(evaluate_neural_models(data_entry, profile_name))\n",
    "\n",
    "    return pd.DataFrame(records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "171b91f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (200000, 31)\n"
     ]
    }
   ],
   "source": [
    "raw_df = load_dataset()\n",
    "PROFILE_DATA_CACHE.clear()\n",
    "print(f'Dataset shape: {raw_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16712796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "950ebf2e12a54eff815329c5a7723fe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P0_baseline:mlp:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5223ba4040784b3e932474c2f24b0b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P0_baseline:lstm:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xj/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4670d78e3764cdfb77a145e91f871e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P0_baseline:transformer:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f615f4b877cb413684b394d1d3ce5f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P1_robust_with_winsorize:mlp:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6dd425e0084fbb94e63112211d5e59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P1_robust_with_winsorize:lstm:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xj/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88282bc3eff74ac0b8ecbcb96b5ed6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P1_robust_with_winsorize:transformer:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "518932d7dd024b8fad2ac71f088ee62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P2_minmax_global:mlp:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1031429787aa43a3bbbca45f14f02937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P2_minmax_global:lstm:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xj/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b35cf81db0a42c8b2024394b10f36a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P2_minmax_global:transformer:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b35eacf252415199cc253308bdac5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P3_log_amount_no_scaling:mlp:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d696c9901a4e1ba0e2553dd470ffd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P3_log_amount_no_scaling:lstm:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xj/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf325af25ba04df7b7bf37095afbcf9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train P3_log_amount_no_scaling:transformer:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile</th>\n",
       "      <th>model</th>\n",
       "      <th>epochs</th>\n",
       "      <th>snapshot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>mlp</td>\n",
       "      <td>20</td>\n",
       "      <td>P0_baseline_mlp.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>lstm</td>\n",
       "      <td>25</td>\n",
       "      <td>P0_baseline_lstm.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>transformer</td>\n",
       "      <td>25</td>\n",
       "      <td>P0_baseline_transformer.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>mlp</td>\n",
       "      <td>20</td>\n",
       "      <td>P1_robust_with_winsorize_mlp.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>lstm</td>\n",
       "      <td>25</td>\n",
       "      <td>P1_robust_with_winsorize_lstm.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>transformer</td>\n",
       "      <td>25</td>\n",
       "      <td>P1_robust_with_winsorize_transformer.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>mlp</td>\n",
       "      <td>20</td>\n",
       "      <td>P2_minmax_global_mlp.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>lstm</td>\n",
       "      <td>25</td>\n",
       "      <td>P2_minmax_global_lstm.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>transformer</td>\n",
       "      <td>25</td>\n",
       "      <td>P2_minmax_global_transformer.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>mlp</td>\n",
       "      <td>20</td>\n",
       "      <td>P3_log_amount_no_scaling_mlp.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>lstm</td>\n",
       "      <td>25</td>\n",
       "      <td>P3_log_amount_no_scaling_lstm.pth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>transformer</td>\n",
       "      <td>25</td>\n",
       "      <td>P3_log_amount_no_scaling_transformer.pth</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     profile        model  epochs  \\\n",
       "0                P0_baseline          mlp      20   \n",
       "1                P0_baseline         lstm      25   \n",
       "2                P0_baseline  transformer      25   \n",
       "3   P1_robust_with_winsorize          mlp      20   \n",
       "4   P1_robust_with_winsorize         lstm      25   \n",
       "5   P1_robust_with_winsorize  transformer      25   \n",
       "6           P2_minmax_global          mlp      20   \n",
       "7           P2_minmax_global         lstm      25   \n",
       "8           P2_minmax_global  transformer      25   \n",
       "9   P3_log_amount_no_scaling          mlp      20   \n",
       "10  P3_log_amount_no_scaling         lstm      25   \n",
       "11  P3_log_amount_no_scaling  transformer      25   \n",
       "\n",
       "                                    snapshot  \n",
       "0                        P0_baseline_mlp.pth  \n",
       "1                       P0_baseline_lstm.pth  \n",
       "2                P0_baseline_transformer.pth  \n",
       "3           P1_robust_with_winsorize_mlp.pth  \n",
       "4          P1_robust_with_winsorize_lstm.pth  \n",
       "5   P1_robust_with_winsorize_transformer.pth  \n",
       "6                   P2_minmax_global_mlp.pth  \n",
       "7                  P2_minmax_global_lstm.pth  \n",
       "8           P2_minmax_global_transformer.pth  \n",
       "9           P3_log_amount_no_scaling_mlp.pth  \n",
       "10         P3_log_amount_no_scaling_lstm.pth  \n",
       "11  P3_log_amount_no_scaling_transformer.pth  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_logs = train_neural_profiles(raw_df, overwrite=False)\n",
    "training_logs_df = pd.DataFrame(training_logs)\n",
    "training_logs_df if not training_logs_df.empty else 'No neural training executed (snapshots already present).'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a1274d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xj/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/home/xj/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/home/xj/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "/home/xj/miniconda3/envs/ML/lib/python3.12/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preproc_profile</th>\n",
       "      <th>model</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_samples</th>\n",
       "      <th>test_samples</th>\n",
       "      <th>num_features</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>lstm</td>\n",
       "      <td>neural</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.997450</td>\n",
       "      <td>0.301370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>mlp</td>\n",
       "      <td>neural</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.978000</td>\n",
       "      <td>0.051724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>transformer</td>\n",
       "      <td>neural</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.978425</td>\n",
       "      <td>0.046409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>decision_tree</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.295082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>gradient_boosting</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999350</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.976450</td>\n",
       "      <td>0.044625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999650</td>\n",
       "      <td>0.740741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P0_baseline</td>\n",
       "      <td>svm</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999500</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>lstm</td>\n",
       "      <td>neural</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.864925</td>\n",
       "      <td>0.009533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>mlp</td>\n",
       "      <td>neural</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.987700</td>\n",
       "      <td>0.092251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>transformer</td>\n",
       "      <td>neural</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.933575</td>\n",
       "      <td>0.017745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>decision_tree</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.998100</td>\n",
       "      <td>0.355932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>gradient_boosting</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.996900</td>\n",
       "      <td>0.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999600</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.983400</td>\n",
       "      <td>0.062147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.769231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>P1_robust_with_winsorize</td>\n",
       "      <td>svm</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999525</td>\n",
       "      <td>0.612245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>lstm</td>\n",
       "      <td>neural</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>mlp</td>\n",
       "      <td>neural</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.911475</td>\n",
       "      <td>0.014473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>transformer</td>\n",
       "      <td>neural</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>decision_tree</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.997825</td>\n",
       "      <td>0.292683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>gradient_boosting</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999350</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.976450</td>\n",
       "      <td>0.044625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999650</td>\n",
       "      <td>0.740741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>P2_minmax_global</td>\n",
       "      <td>svm</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999675</td>\n",
       "      <td>0.754717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>lstm</td>\n",
       "      <td>neural</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.992350</td>\n",
       "      <td>0.130682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>mlp</td>\n",
       "      <td>neural</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>transformer</td>\n",
       "      <td>neural</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>decision_tree</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.997850</td>\n",
       "      <td>0.295082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>gradient_boosting</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999350</td>\n",
       "      <td>0.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999300</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.985950</td>\n",
       "      <td>0.072607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>random_forest</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999650</td>\n",
       "      <td>0.740741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>P3_log_amount_no_scaling</td>\n",
       "      <td>svm</td>\n",
       "      <td>traditional</td>\n",
       "      <td>160000</td>\n",
       "      <td>40000</td>\n",
       "      <td>30</td>\n",
       "      <td>0.999275</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             preproc_profile                model   model_type  train_samples  \\\n",
       "0                P0_baseline                 lstm       neural         160000   \n",
       "1                P0_baseline                  mlp       neural         160000   \n",
       "2                P0_baseline          transformer       neural         160000   \n",
       "3                P0_baseline        decision_tree  traditional         160000   \n",
       "4                P0_baseline    gradient_boosting  traditional         160000   \n",
       "5                P0_baseline  logistic_regression  traditional         160000   \n",
       "6                P0_baseline          naive_bayes  traditional         160000   \n",
       "7                P0_baseline        random_forest  traditional         160000   \n",
       "8                P0_baseline                  svm  traditional         160000   \n",
       "9   P1_robust_with_winsorize                 lstm       neural         160000   \n",
       "10  P1_robust_with_winsorize                  mlp       neural         160000   \n",
       "11  P1_robust_with_winsorize          transformer       neural         160000   \n",
       "12  P1_robust_with_winsorize        decision_tree  traditional         160000   \n",
       "13  P1_robust_with_winsorize    gradient_boosting  traditional         160000   \n",
       "14  P1_robust_with_winsorize  logistic_regression  traditional         160000   \n",
       "15  P1_robust_with_winsorize          naive_bayes  traditional         160000   \n",
       "16  P1_robust_with_winsorize        random_forest  traditional         160000   \n",
       "17  P1_robust_with_winsorize                  svm  traditional         160000   \n",
       "18          P2_minmax_global                 lstm       neural         160000   \n",
       "19          P2_minmax_global                  mlp       neural         160000   \n",
       "20          P2_minmax_global          transformer       neural         160000   \n",
       "21          P2_minmax_global        decision_tree  traditional         160000   \n",
       "22          P2_minmax_global    gradient_boosting  traditional         160000   \n",
       "23          P2_minmax_global  logistic_regression  traditional         160000   \n",
       "24          P2_minmax_global          naive_bayes  traditional         160000   \n",
       "25          P2_minmax_global        random_forest  traditional         160000   \n",
       "26          P2_minmax_global                  svm  traditional         160000   \n",
       "27  P3_log_amount_no_scaling                 lstm       neural         160000   \n",
       "28  P3_log_amount_no_scaling                  mlp       neural         160000   \n",
       "29  P3_log_amount_no_scaling          transformer       neural         160000   \n",
       "30  P3_log_amount_no_scaling        decision_tree  traditional         160000   \n",
       "31  P3_log_amount_no_scaling    gradient_boosting  traditional         160000   \n",
       "32  P3_log_amount_no_scaling  logistic_regression  traditional         160000   \n",
       "33  P3_log_amount_no_scaling          naive_bayes  traditional         160000   \n",
       "34  P3_log_amount_no_scaling        random_forest  traditional         160000   \n",
       "35  P3_log_amount_no_scaling                  svm  traditional         160000   \n",
       "\n",
       "    test_samples  num_features  accuracy        f1  \n",
       "0          40000            30  0.997450  0.301370  \n",
       "1          40000            30  0.978000  0.051724  \n",
       "2          40000            30  0.978425  0.046409  \n",
       "3          40000            30  0.997850  0.295082  \n",
       "4          40000            30  0.999350  0.480000  \n",
       "5          40000            30  0.999300  0.500000  \n",
       "6          40000            30  0.976450  0.044625  \n",
       "7          40000            30  0.999650  0.740741  \n",
       "8          40000            30  0.999500  0.545455  \n",
       "9          40000            30  0.864925  0.009533  \n",
       "10         40000            30  0.987700  0.092251  \n",
       "11         40000            30  0.933575  0.017745  \n",
       "12         40000            30  0.998100  0.355932  \n",
       "13         40000            30  0.996900  0.225000  \n",
       "14         40000            30  0.999600  0.714286  \n",
       "15         40000            30  0.983400  0.062147  \n",
       "16         40000            30  0.999700  0.769231  \n",
       "17         40000            30  0.999525  0.612245  \n",
       "18         40000            30  0.999275  0.000000  \n",
       "19         40000            30  0.911475  0.014473  \n",
       "20         40000            30  0.999275  0.000000  \n",
       "21         40000            30  0.997825  0.292683  \n",
       "22         40000            30  0.999350  0.480000  \n",
       "23         40000            30  0.999275  0.000000  \n",
       "24         40000            30  0.976450  0.044625  \n",
       "25         40000            30  0.999650  0.740741  \n",
       "26         40000            30  0.999675  0.754717  \n",
       "27         40000            30  0.992350  0.130682  \n",
       "28         40000            30  0.999275  0.000000  \n",
       "29         40000            30  0.999275  0.000000  \n",
       "30         40000            30  0.997850  0.295082  \n",
       "31         40000            30  0.999350  0.480000  \n",
       "32         40000            30  0.999300  0.176471  \n",
       "33         40000            30  0.985950  0.072607  \n",
       "34         40000            30  0.999650  0.740741  \n",
       "35         40000            30  0.999275  0.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ablation_results = run_evaluation(raw_df)\n",
    "sort_columns = [col for col in ['preproc_profile', 'model_type', 'model'] if col in ablation_results.columns]\n",
    "if sort_columns:\n",
    "    ablation_results = ablation_results.sort_values(sort_columns)\n",
    "ablation_results.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ee6410",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- Neural model weights and per-epoch loss histories are stored under `assets/snapshots` and re-used on subsequent runs.\n",
    "- Set `overwrite=True` when calling `train_neural_profiles` to retrain snapshots.\n",
    "- Ensure PyTorch with CUDA is installed to take advantage of GPU acceleration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
