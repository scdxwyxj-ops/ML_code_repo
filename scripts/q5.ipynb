{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceb9820f",
   "metadata": {},
   "source": [
    "# Q4 Comparison Between Traditional Method and Machine Learning Methods\n",
    "\n",
    "This notebook experiments with various strategies (Markowitz and several Machine Learning algorithms)  for constructing portfolios to determine the most effective method for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fa0dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import sys\n",
    "cwd = Path(os.getcwd())\n",
    "if cwd.name == 'scripts':\n",
    "    # If running from 'scripts' directory, go up one level\n",
    "    project_root = cwd.parent\n",
    "else:\n",
    "    # Otherwise, assume current directory is the project root\n",
    "    project_root = cwd\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from models.neural import SequenceTransformer\n",
    "from models.neural import SequenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabd614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = project_root / 'assets' / 'datasets' / 'stock_market' / 'Stocks'\n",
    "TICKERS = [\n",
    "    'nvda.us.txt', 'msft.us.txt', 'aapl.us.txt', 'googl.us.txt', 'amzn.us.txt', 'fb.us.txt', 'avgo.us.txt', 'tsla.us.txt', 'tsm.us.txt', 'brk-b.us.txt', \n",
    "    'orcl.us.txt', 'jpm.us.txt', 'wmt.us.txt', 'lly.us.txt', 'v.us.txt', 'nflx.us.txt', 'ma.us.txt', 'xom.us.txt', 'jnj.us.txt', 'cost.us.txt', \n",
    "    'abbv.us.txt', 'asml.us.txt', 'hd.us.txt', 'baba.us.txt', 'bac.us.txt', 'amd.us.txt', 'pg.us.txt', 'unh.us.txt', 'ge.us.txt', 'sap.us.txt', \n",
    "    'cvx.us.txt', 'ko.us.txt', 'csco.us.txt', 'azn.us.txt', 'ibm.us.txt', 'nvo.us.txt', 'tmus.us.txt', 'wfc.us.txt', 'nvs.us.txt', 'tm.us.txt', \n",
    "    'gs.us.txt', 'pm.us.txt', 'ms.us.txt', 'crm.us.txt', 'cat.us.txt', 'abt.us.txt', 'hsbc.us.txt', 'axp.us.txt', 'mu.us.txt', 'mcd.us.txt', \n",
    "    'dis.us.txt', 'tmo.us.txt', 'bx.us.txt', 'now.us.txt', 'anet.us.txt', 't.us.txt', 'intu.us.txt', 'blk.us.txt', 'intc.us.txt', 'c.us.txt', \n",
    "    'amat.us.txt', 'lrcx.us.txt', 'qcom.us.txt', 'nee.us.txt', 'schw.us.txt', 'hdb.us.txt', 'vz.us.txt', 'ba.us.txt', 'txn.us.txt', 'amgn.us.txt', \n",
    "    'tjx.us.txt', 'isrg.us.txt', 'aph.us.txt', 'acn.us.txt', 'ul.us.txt', 'san.us.txt', 'dhr.us.txt', 'gild.us.txt', 'spgi.us.txt', 'etn.us.txt', \n",
    "    'adbe.us.txt'\n",
    "]\n",
    "LIMIT_PER_TICKER = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1457b74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "all_data = []\n",
    "for ticker_file in TICKERS:\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_PATH / ticker_file)\n",
    "        df['Ticker'] = ticker_file.split('.')[0].replace('-', '.') # Handle tickers like brk-b\n",
    "        if LIMIT_PER_TICKER:\n",
    "            df = df.tail(LIMIT_PER_TICKER)\n",
    "        all_data.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {ticker_file} not found. Skipping.\")\n",
    "\n",
    "if not all_data:\n",
    "    raise FileNotFoundError(\"No stock data found. Please check the data path and tickers.\")\n",
    "\n",
    "data = pd.concat(all_data, ignore_index=True)\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data = data.sort_values(by=['Date', 'Ticker']).reset_index(drop=True)\n",
    "\n",
    "print(\"Data loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36eb930c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data['Return'] = data.groupby('Ticker')['Close'].pct_change()\n",
    "data['Target'] = data.groupby('Ticker')['Return'].shift(-1)\n",
    "data = data.dropna()\n",
    "data['Target_Direction'] = (data['Target'] > 0).astype(int)\n",
    "\n",
    "features = ['Return']\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, shuffle=False)\n",
    "\n",
    "print(\"Data preparation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eb54a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional Portfolio Weights calculated.\n"
     ]
    }
   ],
   "source": [
    "def get_portfolio_performance(daily_returns, risk_free_rate=0.04):\n",
    "    annual_return = daily_returns.mean() * 252\n",
    "\n",
    "    annual_volatility = daily_returns.std() * np.sqrt(252)\n",
    "    excess_return = annual_return - risk_free_rate\n",
    "\n",
    "    if annual_volatility != 0:\n",
    "        sharpe_ratio = excess_return / annual_volatility\n",
    "    else:\n",
    "        sharpe_ratio = 0\n",
    "\n",
    "    return annual_return, annual_volatility, sharpe_ratio\n",
    "\n",
    "def get_neg_sharpe_ratio(weights: np.ndarray, mean_returns: pd.Series, cov_matrix: pd.DataFrame) -> float:\n",
    "    \"\"\"Calculates the negative Sharpe ratio for the optimizer.\"\"\"\n",
    "    portfolio_return = np.sum(mean_returns * weights) * 252\n",
    "    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)\n",
    "    sharpe_ratio = portfolio_return / portfolio_std if portfolio_std != 0 else 0\n",
    "    return -sharpe_ratio\n",
    "\n",
    "def find_optimal_weights(mean_returns: pd.Series, cov_matrix: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"Finds the optimal portfolio weights that maximize the Sharpe ratio.\"\"\"\n",
    "    num_assets = len(mean_returns)\n",
    "    args = (mean_returns, cov_matrix)\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bounds = tuple((0, 1) for _ in range(num_assets))\n",
    "    initial_weights = num_assets * [1. / num_assets]\n",
    "\n",
    "    result = minimize(get_neg_sharpe_ratio, initial_weights, args=args,\n",
    "                        method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "    return result.x\n",
    "\n",
    "# Prepare data for traditional optimization\n",
    "train_returns_df = train_data.pivot(index='Date', columns='Ticker', values='Return')\n",
    "mean_returns = train_returns_df.mean()\n",
    "cov_matrix = train_returns_df.cov()\n",
    "\n",
    "# Calculate optimal weights for the traditional portfolio\n",
    "traditional_weights = find_optimal_weights(mean_returns, cov_matrix)\n",
    "traditional_weights = pd.Series(traditional_weights, index=mean_returns.index)\n",
    "\n",
    "print(\"Traditional Portfolio Weights calculated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d90fb4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Classical ML Models ---\n",
      "Training Logistic Regression...\n",
      "--- Logistic Regression Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.02      0.03      3707\n",
      "           1       0.54      0.98      0.70      4361\n",
      "\n",
      "    accuracy                           0.54      8068\n",
      "   macro avg       0.48      0.50      0.36      8068\n",
      "weighted avg       0.49      0.54      0.39      8068\n",
      "\n",
      "Training Random Forest...\n",
      "--- Random Forest Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.47      0.46      3707\n",
      "           1       0.54      0.53      0.53      4361\n",
      "\n",
      "    accuracy                           0.50      8068\n",
      "   macro avg       0.50      0.50      0.50      8068\n",
      "weighted avg       0.50      0.50      0.50      8068\n",
      "\n",
      "Training Gradient Boosting...\n",
      "--- Gradient Boosting Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.26      0.33      3707\n",
      "           1       0.54      0.75      0.63      4361\n",
      "\n",
      "    accuracy                           0.52      8068\n",
      "   macro avg       0.50      0.50      0.48      8068\n",
      "weighted avg       0.51      0.52      0.49      8068\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_data[features])\n",
    "y_train = train_data['Target_Direction']\n",
    "X_test_scaled = scaler.transform(test_data[features])\n",
    "y_test = test_data['Target_Direction']\n",
    "\n",
    "# --- Classical ML Models ---\n",
    "print(\"\\n--- Training Classical ML Models ---\")\n",
    "ml_models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "}\n",
    "ml_predictions = {}\n",
    "for name, model in ml_models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    predictions = model.predict(X_test_scaled)\n",
    "    ml_predictions[name] = predictions\n",
    "    print(f\"--- {name} Classification Report ---\")\n",
    "    print(classification_report(y_test, predictions, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1747ecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Transformer Model ---\n",
      "Early stopping at epoch 16\n",
      "--- Transformer Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      3700\n",
      "           1       0.54      1.00      0.70      4358\n",
      "\n",
      "    accuracy                           0.54      8058\n",
      "   macro avg       0.27      0.50      0.35      8058\n",
      "weighted avg       0.29      0.54      0.38      8058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Deep Learning Model ---\n",
    "print(\"\\n--- Training Transformer Model ---\")\n",
    "\n",
    "def create_sequences(X, y, time_steps=10):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X[i:(i + time_steps)])\n",
    "        ys.append(y[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "TIME_STEPS = 10\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train.values, TIME_STEPS)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test.values, TIME_STEPS)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_seq, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "def train_dl_model(model, train_loader, val_loader, epochs=50, learning_rate=0.001, patience=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "def predict_dl(model, data_loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for features, _ in data_loader:\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.extend(predicted.tolist())\n",
    "    return np.array(predictions)\n",
    "\n",
    "input_dim = len(features)\n",
    "transformer_model = SequenceTransformer(input_dim=input_dim, d_model=64, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.3)\n",
    "\n",
    "train_dl_model(transformer_model, train_loader, val_loader)\n",
    "transformer_predictions_full = predict_dl(transformer_model, DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=128, shuffle=False))\n",
    "dl_predictions = {'Transformer': transformer_predictions_full}\n",
    "\n",
    "print(f\"--- Transformer Classification Report ---\")\n",
    "print(classification_report(y_test_seq, transformer_predictions_full, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e35d8e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_portfolio(test_df, predictions):\n",
    "    # Align predictions with test_df\n",
    "    if len(predictions) < len(test_df):\n",
    "        offset = len(test_df) - len(predictions)\n",
    "        test_df = test_df.iloc[offset:]\n",
    "    \n",
    "    test_df['prediction'] = predictions\n",
    "\n",
    "    daily_returns = []\n",
    "    for date, daily_group in test_df.groupby('Date'):\n",
    "        selected_tickers = daily_group[daily_group['prediction'] == 1]['Ticker'].unique()\n",
    "        if len(selected_tickers) > 0:\n",
    "            daily_return = daily_group[daily_group['Ticker'].isin(selected_tickers)]['Target'].mean()\n",
    "            daily_returns.append(daily_return)\n",
    "        else:\n",
    "            daily_returns.append(0)\n",
    "    return pd.Series(daily_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d22046a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Portfolio Performance Comparison ---\n",
      "                       Annual Return  Volatility  Sharpe Ratio\n",
      "Traditional Markowitz       0.330220    0.104143      2.786735\n",
      "Logistic Regression         0.268434    0.083258      2.743690\n",
      "Random Forest               0.260887    0.086415      2.556120\n",
      "Gradient Boosting           0.254145    0.084847      2.523905\n",
      "Transformer                 0.286766    0.083216      2.965377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d8/b104skn100s1_v0292vkd2yh0000gn/T/ipykernel_68094/214918781.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['prediction'] = predictions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_results = {}\n",
    "\n",
    "# --- Traditional Markowitz ---\n",
    "test_returns_pivot = test_data.pivot(index='Date', columns='Ticker', values='Return')\n",
    "# Align weights with test returns columns\n",
    "aligned_weights = traditional_weights.reindex(test_returns_pivot.columns).fillna(0)\n",
    "daily_trad_returns = (test_returns_pivot * aligned_weights).sum(axis=1)\n",
    "all_results['Traditional Markowitz'] = get_portfolio_performance(daily_trad_returns)\n",
    "\n",
    "# --- ML Models ---\n",
    "for name, predictions in ml_predictions.items():\n",
    "    daily_ml_returns = backtest_portfolio(test_data, predictions)\n",
    "    all_results[name] = get_portfolio_performance(daily_ml_returns)\n",
    "\n",
    "# --- DL Model ---\n",
    "# Align DL predictions. They are shorter due to sequence creation.\n",
    "dl_test_data = test_data.iloc[len(test_data) - len(transformer_predictions_full):]\n",
    "daily_dl_returns = backtest_portfolio(dl_test_data, transformer_predictions_full)\n",
    "all_results['Transformer'] = get_portfolio_performance(daily_dl_returns)\n",
    "\n",
    "# --- Display Results ---\n",
    "final_results_df = pd.DataFrame.from_dict(all_results, orient='index', columns=['Annual Return', 'Volatility', 'Sharpe Ratio'])\n",
    "print(\"\\n--- Final Portfolio Performance Comparison ---\")\n",
    "print(final_results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
